{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will show how to train a complete models in some popular tools: random forest regressor, xgboost and lightGBM\n",
    "\n",
    "    Instructor: Yimin Nie\n",
    "    Email: ymnie888@gmail.com\n",
    "    \n",
    "    In the notebook, I show you the entire pipeline using taxi trip data set, and show how to put all workable codes into \n",
    "    a python project to run your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import useful libs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import math\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to reduce dataframe memory footprint, reduce float and int to the minimum dtype\n",
    "def reducemem(self):\n",
    "    for c in self:\n",
    "        if self[c].dtype =='int64':\n",
    "            if self[c].max()<np.iinfo(np.int32).max and self[c].min()>np.iinfo(np.int32).min:\n",
    "                self[c]=self[c].astype(np.int32)           \n",
    "            if self[c].max()<np.iinfo(np.int16).max and self[c].min()>np.iinfo(np.int16).min:\n",
    "                self[c]=self[c].astype(np.int16)\n",
    "            if self[c].max()<np.iinfo(np.int8).max and self[c].min()>np.iinfo(np.int8).min:\n",
    "                self[c]=self[c].astype(np.int8) \n",
    "                \n",
    "        if self[c].dtype =='float64':\n",
    "            if self[c].max()<np.finfo(np.float32).max and self[c].min()>np.finfo(np.float32).min:\n",
    "                self[c]=self[c].astype(np.float32)           \n",
    "            if self[c].max()<np.finfo(np.float16).max and self[c].min()>np.finfo(np.float16).min:\n",
    "                self[c]=self[c].astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START\n",
    "# IMPORT DATA TYPE DICTIONARY csv\n",
    "\n",
    "with open('ctdict2.pkl', 'rb') as handle:\n",
    "    cvsdict = pickle.load(handle)\n",
    "\n",
    "df = pd.read_csv('df_application_train_new2.csv',dtype=cvsdict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. process the data and extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.replace(-np.Inf, 0)\n",
    "df=df.replace(np.Inf, 0)\n",
    "df=df.replace(np.nan, 0)\n",
    "def trim(st):\n",
    "    st=st.strip(\" \")\n",
    "    st=st.replace(',', '')\n",
    "    st=st.replace(' ', '_')\n",
    "    st=st.replace(':', '_')\n",
    "    return st\n",
    "df = df.rename(columns=lambda x: trim(x))\n",
    "reducemem(df)\n",
    "gc.collect()\n",
    "y_train=[]\n",
    "X_train=[]\n",
    "\n",
    "def allsample(df):\n",
    "    y_train = df['TARGET']\n",
    "    X_train = df.drop(columns=['TARGET'])\n",
    "    del  df\n",
    "    return(X_train , y_train)\n",
    "\n",
    "def equaltargetsplit(df):    \n",
    "    x1=df[df['TARGET']==1]\n",
    "    x0=df[df['TARGET']==0]\n",
    "    x0=x0.sample(n=x1.shape[0], replace=True, random_state=1)\n",
    "    x=pd.concat([x1,x0],axis=0).reset_index(drop=True)\n",
    "    y_train=x['TARGET']\n",
    "    X_train=x.drop(columns=['TARGET'])\n",
    "    del df,x1,x0,x\n",
    "    return(X_train , y_train)\n",
    "\n",
    "#call your sampling method \n",
    "X_train , y_train = allsample(df)\n",
    "#X_train , y_train = equaltargetsplit(df)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build your models\n",
    "\n",
    "\n",
    "    before building your models, make sure \n",
    "        (1) your target ( regression or classification)\n",
    "        (2) evaluation metric in terms of your target\n",
    "        (3) how to train your model (here I use 5-fold cross validation)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lgb():\n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'silent':-1,\n",
    "    \"max_depth\": 20,\n",
    "    \"num_leaves\": 250,\n",
    "    \"max_bin\": 2500,\n",
    "    \"n_estimators\": 50000\n",
    "}\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    return model\n",
    "\n",
    "def model_xgb():\n",
    "    model = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                     gamma=0,                 \n",
    "                     learning_rate=0.07,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=1.5,\n",
    "                     n_estimators=10000,                                                                    \n",
    "                     reg_alpha=0.75,\n",
    "                     reg_lambda=0.45,\n",
    "                     subsample=0.6,\n",
    "                     seed=42\n",
    "                ) \n",
    "    return model\n",
    "\n",
    "def model_rf():\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=config.n_estimator,\n",
    "        max_depth = config.max_depth,\n",
    "        random_state=config.seed,\n",
    "        n_jobs=config.n_jobs,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's auc: 0.884712\ttraining's binary_logloss: 0.207723\tvalid_1's auc: 0.76691\tvalid_1's binary_logloss: 0.242939\n",
      "[400]\ttraining's auc: 0.945511\ttraining's binary_logloss: 0.176518\tvalid_1's auc: 0.777762\tvalid_1's binary_logloss: 0.23814\n",
      "[600]\ttraining's auc: 0.974707\ttraining's binary_logloss: 0.153694\tvalid_1's auc: 0.78238\tvalid_1's binary_logloss: 0.236516\n",
      "[800]\ttraining's auc: 0.988598\ttraining's binary_logloss: 0.135554\tvalid_1's auc: 0.784364\tvalid_1's binary_logloss: 0.235885\n",
      "[1000]\ttraining's auc: 0.994883\ttraining's binary_logloss: 0.120789\tvalid_1's auc: 0.785131\tvalid_1's binary_logloss: 0.235761\n",
      "[1200]\ttraining's auc: 0.997781\ttraining's binary_logloss: 0.108158\tvalid_1's auc: 0.7858\tvalid_1's binary_logloss: 0.235974\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "kf = KFold(5)\n",
    "cv_scores = []\n",
    "model_name = 'lgb'\n",
    "for i, (tr_idx, vl_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('FOLD {} \\n'.format(i))\n",
    "    X_tr, y_tr = X_train.loc[tr_idx], y_train[tr_idx]\n",
    "    X_vl, y_vl = X_train.loc[vl_idx], y_train[vl_idx]\n",
    "\n",
    "    if model_name == 'lgb':\n",
    "        model = model_lgb()\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_vl, y_vl)], \\\n",
    "                  eval_metric='auc', verbose=200, early_stopping_rounds=500)\n",
    "        with open('lgb_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'rf':\n",
    "        model = model_rf()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'xgb':\n",
    "        model = model_xgb()\n",
    "        train_data  = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        valid_data  = xgb.DMatrix(X_vl, label=y_vl)\n",
    "        evallist = [(train_data, 'train'), (valid_data, 'valid')]\n",
    "        parms = {'max_depth':15, #maximum depth of a tree 8 12\n",
    "         'objective':'reg:linear',\n",
    "         'eta'      :0.05, #0.3\n",
    "         'subsample':0.9,#SGD will use this percentage of data 0.8 0.99\n",
    "         'lambda '  :3, #L2 regularization term,>1 more conservative 4 \n",
    "         'colsample_bytree ':0.6, #0.9\n",
    "         'colsample_bylevel':0.7, #1 0.7\n",
    "         'min_child_weight': 0.5, #10 0.5\n",
    "         #'nthread'  :3 ... default is max cores\n",
    "         'eval_metric':'rmse'}  #number of cpu core to use\n",
    "        # running for 2k iterations \n",
    "        model = xgb.train(parms, train_data, num_boost_round=2000, evals = evallist,\n",
    "                          early_stopping_rounds=50, maximize=False, \n",
    "                          verbose_eval=100)\n",
    "#         model.fit(X_tr, y_tr,eval_set=(X_vl, y_vl))\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
